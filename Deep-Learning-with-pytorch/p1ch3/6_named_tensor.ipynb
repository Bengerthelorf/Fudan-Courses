{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "_ = torch.tensor([0.2126, 0.7152, 0.0722], names=['c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ‘†\n",
    "\n",
    "`/tmp/ipykernel_141638/1646494204.py:2: ç”¨æˆ·è­¦å‘Šï¼šå‘½åå¼ é‡åŠå…¶æ‰€æœ‰ç›¸å…³çš„APIéƒ½æ˜¯å®éªŒæ€§åŠŸèƒ½ï¼Œå¯èƒ½ä¼šå‘ç”Ÿå˜åŒ–ã€‚åœ¨å®ƒä»¬è¢«å‘å¸ƒä¸ºç¨³å®šç‰ˆæœ¬ä¹‹å‰ï¼Œè¯·ä¸è¦ç”¨å®ƒä»¬æ¥åšä»»ä½•é‡è¦çš„äº‹æƒ…ã€‚ï¼ˆåœ¨../c10/core/TensorImpl.h:1788å†…éƒ¨è§¦å‘ï¼‰ _ = torch.tensor([0.2126, 0.7152, 0.0722], names=['c'])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t = torch.randn(3, 5, 5)\n",
    "weights = torch.tensor([0.2126, 0.71152, 0.0722])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`img_t = torch.randn(3, 5, 5)` \n",
    "\n",
    "shape: [channels, rows, columns]\n",
    "\n",
    "channels: RGB\n",
    "\n",
    "---\n",
    "\n",
    "`weights = torch.tensor([0.2126, 0.7152, 0.0722])`\n",
    "\n",
    "å…¶ä¸­, 0.2126, 0.7152, 0.0722 æ˜¯ RGB ä¸‰ä¸ªé€šé“çš„æƒé‡\n",
    "\n",
    "> - 0.2126: çº¢è‰²\n",
    "> - 0.7152: ç»¿è‰²\n",
    "> - 0.0722: è“è‰²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_t = torch.randn(2, 3, 5, 5) # shape [batch, channels, rows, columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`[å¤šå°‘å¼ å›¾ç‰‡, é€šé“æ•°, è¡Œ, åˆ—]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æŠŠä¸‰ç»´å›¾ç‰‡è½¬æ¢æˆä¸€ç»´(RGB ğŸ‘‰ ç°åº¦)\n",
    "\n",
    "### å¹³å‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), torch.Size([2, 5, 5]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_naive = img_t.mean(-3)  # æ—¢æ˜¯æŠŠ 'é€šé“' ç»´åº¦çš„ RGB ä¸‰ä¸ªæ•°æ±‚å¹³å‡\n",
    "batch_gray_naive = batch_t.mean(-3)\n",
    "img_gray_naive.shape, batch_gray_naive.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ‘†\n",
    "\n",
    "å¯¹äºä¸€ä¸ªå€’æ•°ç¬¬ä¸‰ä¸ªç»´åº¦å–å¹³å‡ä¹‹åå°±å˜æˆäº†ä¸€ä¸ª0ç»´çš„, ç›¸å½“äºæ˜¯, ä¸€ä¸ªä¸€ç»´çš„å‘é‡å–å¹³å‡ä¹‹åå°±å˜æˆ0ç»´çš„äº†\n",
    "\n",
    "---\n",
    "\n",
    "### åŠ æƒå¹³å‡\n",
    "\n",
    "è§‚å¯Ÿ `weights = torch.tensor([0.2126, 0.71152, 0.0722])`, å¯¹æ­¤, å…¶ä¸ºä¸€ç»´çš„, è¿™ä¸èƒ½ç›´æ¥è¿›è¡Œç®€å•çš„ç›¸åŠ , ç”±æ­¤, éœ€è¦ç”¨åˆ°ä¹‹å‰çš„ `unsqueeze` æ–¹æ³•, å°†å…¶å˜ä¸ºå’Œå›¾ç‰‡ä¸€æ ·çš„ä¸‰ç»´çš„, å†è¿›è¡Œç›¸å…³å¤„ç†."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsqueezed_weights = weights.unsqueeze(-1).unsqueeze(-1)\n",
    "unsqueezed_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ‘†\n",
    "\n",
    "#### P.S.\n",
    "\n",
    "`unsqueeze()` å’Œ `unsqueeze_()` åŒºåˆ«:\n",
    "\n",
    "- `unsqueeze()` è¿”å›ä¸€ä¸ªæ–°çš„å¼ é‡, è€Œ `unsqueeze_()` åˆ™æ˜¯ç›´æ¥åœ¨åŸæ¥çš„å¼ é‡ä¸Šè¿›è¡Œæ“ä½œ, ä¹Ÿå°±æ˜¯è¯´ `unsqueeze()` ä¼šå ç”¨æ›´å¤šçš„å†…å­˜ç©ºé—´, è€Œ `unsqueeze_()` åˆ™ä¸ä¼š. æ‰€ä»¥ `unsqueeze_()` å†åŸåœ°æ“ä½œ (In-place) çš„æ—¶å€™, ä¼šæ›´åŠ é«˜æ•ˆ.\n",
    "- åŒæ ·çš„, å¦‚æœæƒ³è¦å°†ä¿®æ”¹åçš„å¼ é‡èµ‹å€¼ç»™åˆ«çš„å˜é‡, åˆ™éœ€è¦ä½¿ç”¨ `unsqueeze()`, è€Œä¸èƒ½ä½¿ç”¨ `unsqueeze_()`\n",
    "\n",
    "å¯¹æ­¤, æˆ‘ä»¬å¯ä»¥ç®€å•çš„ç”¨ä¸‹é¢çš„ä»£ç è¿›è¡Œæµ‹è¯•: \n",
    "\n",
    "ğŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ True, False, False],\n",
       "         [False,  True, False],\n",
       "         [False, False,  True]]),\n",
       " tensor([[True],\n",
       "         [True],\n",
       "         [True]]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unsqueeze()\n",
    "a = weights.clone()\n",
    "b = a.unsqueeze(-1)\n",
    "\n",
    "# unsqueeze_()\n",
    "c = weights.clone()\n",
    "d = c.unsqueeze_(-1)\n",
    "a == b, c == d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 5, 5]), torch.Size([2, 3, 5, 5]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_weights = (img_t * unsqueezed_weights)\n",
    "batch_weigts = (batch_t * unsqueezed_weights)\n",
    "img_weights.shape, batch_weigts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), torch.Size([2, 5, 5]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_weighted = img_weights.sum(-3)\n",
    "batch_gray_weighted = batch_weigts.sum(-3)\n",
    "img_gray_weighted.shape, batch_gray_weighted.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### çˆ±å› æ–¯å¦æ±‚å’Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), torch.Size([2, 5, 5]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_weighted_fancy = torch.einsum('...chw, c-> ...hw', img_t, weights)\n",
    "batch_gray_weighted_fancy = torch.einsum('...chw, c-> ...hw', batch_t, weights)\n",
    "img_gray_weighted_fancy.shape, batch_gray_weighted_fancy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ‘†\n",
    "\n",
    "`...chw`, æ˜¯å€’ç€çœ‹çš„, ä¹Ÿå°±æ˜¯ä»åå¾€å‰æ•°, æ„æ€å°±æ˜¯å°±ç®— `...` ä¹‹å‰è¿˜æœ‰ä¸€ä¸ªç»´åº¦ä¹Ÿä¸éœ€è¦å†™å‡ºæ¥, è€Œæ˜¯ä¾æ—§ä»åå¾€å‰çš„ `chw`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å‘½å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2126, 0.7152, 0.0772], names=('channels',))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_named = torch.tensor([0.2126, 0.7152, 0.0772], names = ['channels'])\n",
    "weights_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_named = img_t.refine_names(..., 'channels', 'rows', 'columns')\n",
    "batch_named = batch_t.refine_names(..., 'channels', 'rows', 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6416,  0.1317, -1.0025, -0.5073,  0.3314],\n",
       "         [-1.0648,  2.0866,  0.4873,  0.2159,  0.5916],\n",
       "         [ 0.9874,  0.8971,  0.0078,  0.7424, -1.3605],\n",
       "         [-0.0361, -0.2031,  1.0998,  0.7136, -0.4406],\n",
       "         [ 1.0139,  1.4851, -1.2603, -1.5902, -0.2751]],\n",
       "\n",
       "        [[ 2.9091,  0.0371, -0.3771, -0.0621,  0.1385],\n",
       "         [-0.6467,  1.8324,  1.4403,  1.0526,  1.4508],\n",
       "         [ 1.1958, -1.2200,  1.6275,  1.0159, -0.6355],\n",
       "         [ 0.9764,  0.3040, -0.9065,  1.4711,  2.9472],\n",
       "         [ 1.6011,  0.1137, -0.4445,  0.7469,  0.0827]],\n",
       "\n",
       "        [[ 0.0204,  0.6514, -1.3760, -0.6822,  1.0084],\n",
       "         [ 0.7725,  0.2640, -0.1007,  1.2703, -0.1107],\n",
       "         [ 0.1386, -1.8214, -1.9864,  1.3183,  0.5828],\n",
       "         [ 0.0051, -1.8912,  0.7801, -1.8244, -0.8489],\n",
       "         [-1.1395,  0.2634, -0.6095, -0.9424, -0.3488]]],\n",
       "       names=('channels', 'rows', 'columns'))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.4327, -0.6534, -0.7317, -0.4231, -1.2641],\n",
       "          [-2.5323, -0.6371, -0.0243,  0.7110, -1.0352],\n",
       "          [ 2.6752, -0.2881, -0.2490,  2.1944, -0.7277],\n",
       "          [-0.7770,  0.2503, -1.7195,  0.3949,  0.8570],\n",
       "          [-0.8717,  1.3692,  0.5090, -1.6185,  0.7126]],\n",
       "\n",
       "         [[-1.5003,  0.3588, -0.0232,  0.5664, -0.2286],\n",
       "          [-1.5466, -0.5515, -0.7599,  0.3978, -0.9340],\n",
       "          [-0.6201,  0.7426,  0.6493, -0.1922, -2.0722],\n",
       "          [-0.0190, -0.5018,  0.9953,  0.6356,  0.4542],\n",
       "          [-0.3126,  0.2169, -1.5904,  0.6665,  0.7936]],\n",
       "\n",
       "         [[-0.8280,  0.3386, -0.0721,  1.4033,  0.3494],\n",
       "          [-0.1538,  0.8704,  0.9939,  1.7908, -0.3326],\n",
       "          [ 2.1390,  0.5973,  1.3204, -0.0993,  1.6589],\n",
       "          [ 1.1751, -0.7440, -1.2444,  0.0924,  0.1416],\n",
       "          [-0.6961,  2.0161, -2.1123,  1.4692,  0.2306]]],\n",
       "\n",
       "\n",
       "        [[[-1.0430, -0.0069,  0.2654,  0.6894, -0.3216],\n",
       "          [-0.2226,  0.9742,  0.3295,  0.3270, -0.2413],\n",
       "          [ 0.5690,  1.1849, -0.4242,  1.3478,  0.0815],\n",
       "          [ 0.1069, -0.0535, -0.4643, -0.9534,  0.9699],\n",
       "          [-1.0475,  1.8034,  0.4661, -1.5808, -1.2381]],\n",
       "\n",
       "         [[ 0.1528,  0.2327,  0.0111, -0.1323, -1.8104],\n",
       "          [-0.6769, -0.4845, -0.6868, -0.4817, -0.3159],\n",
       "          [-0.5673, -0.5303,  1.3611, -0.3371, -0.4099],\n",
       "          [ 0.7071,  0.9989,  1.1949, -2.4197, -0.9248],\n",
       "          [ 0.3584,  1.3240, -0.8313,  0.0789,  0.2453]],\n",
       "\n",
       "         [[ 1.2063, -0.5607,  0.0309, -0.5904, -1.2867],\n",
       "          [-0.6949, -0.3146, -0.1335, -1.2926, -0.4515],\n",
       "          [-2.1342,  1.3298,  0.2882,  0.3345,  0.8804],\n",
       "          [-0.3030,  1.9416, -0.1670, -0.0040, -2.0968],\n",
       "          [-1.6911, -0.0320, -0.8715,  2.0418,  0.2609]]]],\n",
       "       names=(None, 'channels', 'rows', 'columns'))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1, 1]), ('channels', 'rows', 'columns'))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_aligned = weights_named.align_as(img_named)\n",
    "weights_aligned.shape, weights_aligned.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ‘†\n",
    "\n",
    "å¯¹äºå·²å‘½åçš„å¼ é‡, ä¹Ÿå·²é€šè¿‡ `align_as` æ¥è®©ç»´åº¦å¯¹é½, è®©ç»´åº¦æ›´ä½çš„å‘ç»´åº¦é«˜çš„å¯¹é½. ä¹‹åå°±å¯ä»¥è¿›è¡Œå’Œä¹‹å‰ç›¸åŒçš„æ“ä½œäº†."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 5, 5]), torch.Size([2, 3, 5, 5]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_weighted_named = img_named * weights_aligned\n",
    "batch_gray_weighted_named = batch_named * weights_aligned\n",
    "img_gray_weighted_named.shape, batch_gray_weighted_named.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P.S.\n",
    "\n",
    "ä»¥ä¸‹ä¸ºé”™è¯¯ç¤ºèŒƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when attempting to broadcast dims ['channels', 'rows', 'columns'] and dims ['channels']: dim 'columns' and dim 'channels' are at the same position from the right but do not match.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    gray_named = (img_named[..., :3] * weights_named).sum('channels')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å–æ¶ˆå‘½å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 5, 5]),\n",
       " (None, None, None),\n",
       " torch.Size([2, 3, 5, 5]),\n",
       " (None, None, None, None))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_weighted_plain = img_gray_weighted_named.rename(None)\n",
    "batch_gray_weighted_plain = batch_gray_weighted_named.rename(None)\n",
    "img_gray_weighted_plain.shape, img_gray_weighted_plain.names, batch_gray_weighted_plain.shape, batch_gray_weighted_plain.names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
