{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "_ = torch.tensor([0.2126, 0.7152, 0.0722], names=['c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👆\n",
    "\n",
    "`/tmp/ipykernel_141638/1646494204.py:2: 用户警告：命名张量及其所有相关的API都是实验性功能，可能会发生变化。在它们被发布为稳定版本之前，请不要用它们来做任何重要的事情。（在../c10/core/TensorImpl.h:1788内部触发） _ = torch.tensor([0.2126, 0.7152, 0.0722], names=['c'])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t = torch.randn(3, 5, 5)\n",
    "weights = torch.tensor([0.2126, 0.71152, 0.0722])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`img_t = torch.randn(3, 5, 5)` \n",
    "\n",
    "shape: [channels, rows, columns]\n",
    "\n",
    "channels: RGB\n",
    "\n",
    "---\n",
    "\n",
    "`weights = torch.tensor([0.2126, 0.7152, 0.0722])`\n",
    "\n",
    "其中, 0.2126, 0.7152, 0.0722 是 RGB 三个通道的权重\n",
    "\n",
    "> - 0.2126: 红色\n",
    "> - 0.7152: 绿色\n",
    "> - 0.0722: 蓝色"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_t = torch.randn(2, 3, 5, 5) # shape [batch, channels, rows, columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`[多少张图片, 通道数, 行, 列]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 把三维图片转换成一维(RGB 👉 灰度)\n",
    "\n",
    "### 平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), torch.Size([2, 5, 5]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_naive = img_t.mean(-3)  # 既是把 '通道' 维度的 RGB 三个数求平均\n",
    "batch_gray_naive = batch_t.mean(-3)\n",
    "img_gray_naive.shape, batch_gray_naive.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👆\n",
    "\n",
    "对于一个倒数第三个维度取平均之后就变成了一个0维的, 相当于是, 一个一维的向量取平均之后就变成0维的了\n",
    "\n",
    "---\n",
    "\n",
    "### 加权平均\n",
    "\n",
    "观察 `weights = torch.tensor([0.2126, 0.71152, 0.0722])`, 对此, 其为一维的, 这不能直接进行简单的相加, 由此, 需要用到之前的 `unsqueeze` 方法, 将其变为和图片一样的三维的, 再进行相关处理."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsqueezed_weights = weights.unsqueeze(-1).unsqueeze(-1)\n",
    "unsqueezed_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👆\n",
    "\n",
    "#### P.S.\n",
    "\n",
    "`unsqueeze()` 和 `unsqueeze_()` 区别:\n",
    "\n",
    "- `unsqueeze()` 返回一个新的张量, 而 `unsqueeze_()` 则是直接在原来的张量上进行操作, 也就是说 `unsqueeze()` 会占用更多的内存空间, 而 `unsqueeze_()` 则不会. 所以 `unsqueeze_()` 再原地操作 (In-place) 的时候, 会更加高效.\n",
    "- 同样的, 如果想要将修改后的张量赋值给别的变量, 则需要使用 `unsqueeze()`, 而不能使用 `unsqueeze_()`\n",
    "\n",
    "对此, 我们可以简单的用下面的代码进行测试: \n",
    "\n",
    "👇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ True, False, False],\n",
       "         [False,  True, False],\n",
       "         [False, False,  True]]),\n",
       " tensor([[True],\n",
       "         [True],\n",
       "         [True]]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unsqueeze()\n",
    "a = weights.clone()\n",
    "b = a.unsqueeze(-1)\n",
    "\n",
    "# unsqueeze_()\n",
    "c = weights.clone()\n",
    "d = c.unsqueeze_(-1)\n",
    "a == b, c == d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 5, 5]), torch.Size([2, 3, 5, 5]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_weights = (img_t * unsqueezed_weights)\n",
    "batch_weigts = (batch_t * unsqueezed_weights)\n",
    "img_weights.shape, batch_weigts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), torch.Size([2, 5, 5]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_weighted = img_weights.sum(-3)\n",
    "batch_gray_weighted = batch_weigts.sum(-3)\n",
    "img_gray_weighted.shape, batch_gray_weighted.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爱因斯坦求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), torch.Size([2, 5, 5]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_weighted_fancy = torch.einsum('...chw, c-> ...hw', img_t, weights)\n",
    "batch_gray_weighted_fancy = torch.einsum('...chw, c-> ...hw', batch_t, weights)\n",
    "img_gray_weighted_fancy.shape, batch_gray_weighted_fancy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👆\n",
    "\n",
    "`...chw`, 是倒着看的, 也就是从后往前数, 意思就是就算 `...` 之前还有一个维度也不需要写出来, 而是依旧从后往前的 `chw`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 命名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2126, 0.7152, 0.0772], names=('channels',))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_named = torch.tensor([0.2126, 0.7152, 0.0772], names = ['channels'])\n",
    "weights_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_named = img_t.refine_names(..., 'channels', 'rows', 'columns')\n",
    "batch_named = batch_t.refine_names(..., 'channels', 'rows', 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6416,  0.1317, -1.0025, -0.5073,  0.3314],\n",
       "         [-1.0648,  2.0866,  0.4873,  0.2159,  0.5916],\n",
       "         [ 0.9874,  0.8971,  0.0078,  0.7424, -1.3605],\n",
       "         [-0.0361, -0.2031,  1.0998,  0.7136, -0.4406],\n",
       "         [ 1.0139,  1.4851, -1.2603, -1.5902, -0.2751]],\n",
       "\n",
       "        [[ 2.9091,  0.0371, -0.3771, -0.0621,  0.1385],\n",
       "         [-0.6467,  1.8324,  1.4403,  1.0526,  1.4508],\n",
       "         [ 1.1958, -1.2200,  1.6275,  1.0159, -0.6355],\n",
       "         [ 0.9764,  0.3040, -0.9065,  1.4711,  2.9472],\n",
       "         [ 1.6011,  0.1137, -0.4445,  0.7469,  0.0827]],\n",
       "\n",
       "        [[ 0.0204,  0.6514, -1.3760, -0.6822,  1.0084],\n",
       "         [ 0.7725,  0.2640, -0.1007,  1.2703, -0.1107],\n",
       "         [ 0.1386, -1.8214, -1.9864,  1.3183,  0.5828],\n",
       "         [ 0.0051, -1.8912,  0.7801, -1.8244, -0.8489],\n",
       "         [-1.1395,  0.2634, -0.6095, -0.9424, -0.3488]]],\n",
       "       names=('channels', 'rows', 'columns'))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.4327, -0.6534, -0.7317, -0.4231, -1.2641],\n",
       "          [-2.5323, -0.6371, -0.0243,  0.7110, -1.0352],\n",
       "          [ 2.6752, -0.2881, -0.2490,  2.1944, -0.7277],\n",
       "          [-0.7770,  0.2503, -1.7195,  0.3949,  0.8570],\n",
       "          [-0.8717,  1.3692,  0.5090, -1.6185,  0.7126]],\n",
       "\n",
       "         [[-1.5003,  0.3588, -0.0232,  0.5664, -0.2286],\n",
       "          [-1.5466, -0.5515, -0.7599,  0.3978, -0.9340],\n",
       "          [-0.6201,  0.7426,  0.6493, -0.1922, -2.0722],\n",
       "          [-0.0190, -0.5018,  0.9953,  0.6356,  0.4542],\n",
       "          [-0.3126,  0.2169, -1.5904,  0.6665,  0.7936]],\n",
       "\n",
       "         [[-0.8280,  0.3386, -0.0721,  1.4033,  0.3494],\n",
       "          [-0.1538,  0.8704,  0.9939,  1.7908, -0.3326],\n",
       "          [ 2.1390,  0.5973,  1.3204, -0.0993,  1.6589],\n",
       "          [ 1.1751, -0.7440, -1.2444,  0.0924,  0.1416],\n",
       "          [-0.6961,  2.0161, -2.1123,  1.4692,  0.2306]]],\n",
       "\n",
       "\n",
       "        [[[-1.0430, -0.0069,  0.2654,  0.6894, -0.3216],\n",
       "          [-0.2226,  0.9742,  0.3295,  0.3270, -0.2413],\n",
       "          [ 0.5690,  1.1849, -0.4242,  1.3478,  0.0815],\n",
       "          [ 0.1069, -0.0535, -0.4643, -0.9534,  0.9699],\n",
       "          [-1.0475,  1.8034,  0.4661, -1.5808, -1.2381]],\n",
       "\n",
       "         [[ 0.1528,  0.2327,  0.0111, -0.1323, -1.8104],\n",
       "          [-0.6769, -0.4845, -0.6868, -0.4817, -0.3159],\n",
       "          [-0.5673, -0.5303,  1.3611, -0.3371, -0.4099],\n",
       "          [ 0.7071,  0.9989,  1.1949, -2.4197, -0.9248],\n",
       "          [ 0.3584,  1.3240, -0.8313,  0.0789,  0.2453]],\n",
       "\n",
       "         [[ 1.2063, -0.5607,  0.0309, -0.5904, -1.2867],\n",
       "          [-0.6949, -0.3146, -0.1335, -1.2926, -0.4515],\n",
       "          [-2.1342,  1.3298,  0.2882,  0.3345,  0.8804],\n",
       "          [-0.3030,  1.9416, -0.1670, -0.0040, -2.0968],\n",
       "          [-1.6911, -0.0320, -0.8715,  2.0418,  0.2609]]]],\n",
       "       names=(None, 'channels', 'rows', 'columns'))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1, 1]), ('channels', 'rows', 'columns'))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_aligned = weights_named.align_as(img_named)\n",
    "weights_aligned.shape, weights_aligned.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👆\n",
    "\n",
    "对于已命名的张量, 也已通过 `align_as` 来让维度对齐, 让维度更低的向维度高的对齐. 之后就可以进行和之前相同的操作了."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 5, 5]), torch.Size([2, 3, 5, 5]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_weighted_named = img_named * weights_aligned\n",
    "batch_gray_weighted_named = batch_named * weights_aligned\n",
    "img_gray_weighted_named.shape, batch_gray_weighted_named.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P.S.\n",
    "\n",
    "以下为错误示范"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when attempting to broadcast dims ['channels', 'rows', 'columns'] and dims ['channels']: dim 'columns' and dim 'channels' are at the same position from the right but do not match.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    gray_named = (img_named[..., :3] * weights_named).sum('channels')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 取消命名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 5, 5]),\n",
       " (None, None, None),\n",
       " torch.Size([2, 3, 5, 5]),\n",
       " (None, None, None, None))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_weighted_plain = img_gray_weighted_named.rename(None)\n",
    "batch_gray_weighted_plain = batch_gray_weighted_named.rename(None)\n",
    "img_gray_weighted_plain.shape, img_gray_weighted_plain.names, batch_gray_weighted_plain.shape, batch_gray_weighted_plain.names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
