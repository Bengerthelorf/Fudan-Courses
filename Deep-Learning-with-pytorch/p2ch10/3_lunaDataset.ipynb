{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import csv\n",
    "import functools\n",
    "import glob\n",
    "import os\n",
    "import collections\n",
    "\n",
    "from collections import namedtuple\n",
    "from itertools import chain\n",
    "\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.cuda\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Util`\n",
    "\n",
    "---\n",
    "\n",
    "### **# 1** `from util.disk import getCache`\n",
    "\n",
    "`getCache()` 函数的原码如下:\n",
    "\n",
    "```python\n",
    "def getCache(scope_str):\n",
    "    return FanoutCache('data-unversioned/cache/' + scope_str,\n",
    "                       disk=GzipDisk,\n",
    "                       shards=64,\n",
    "                       timeout=1,\n",
    "                       size_limit=3e11,\n",
    "                       # disk_min_file_size=2**20,\n",
    "                       )\n",
    "```\n",
    "\n",
    "其依赖关系包括: `FanoutCache` 和 `GzipDisk`, 其中, `FanoutCache` 可以从 `diskcache` 中导入, 而 `GzipDisk` 需要我们重新定义.\n",
    "\n",
    "---\n",
    "\n",
    "#### **# 1.1** `FanoutCache`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diskcache import FanoutCache, Disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👆\n",
    "\n",
    "对于此 `getCache()` 函数, 好像并不是很必要, 所以暂且按下不表."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **# 2** `from util.util import XyzTuple, xyz2irc`\n",
    "\n",
    "#### **# 2.1** `XyzTuple`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "XyzTuple = collections.namedtuple('XyzTuple', ['x', 'y', 'z'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **# 2.2** `xyz2irc`\n",
    "\n",
    "其中其依赖 `IrcTuple`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IrcTuple = collections.namedtuple('IrcTuple', ['index', 'row', 'col'])\n",
    "\n",
    "def xyz2irc(coord_xyz, origin_xyz, vxSize_xyz, direction_a):\n",
    "    origin_a = np.array(origin_xyz)\n",
    "    vxSize_a = np.array(vxSize_xyz)\n",
    "    coord_a = np.array(coord_xyz)\n",
    "    cri_a = ((coord_a - origin_a) @ np.linalg.inv(direction_a)) / vxSize_a\n",
    "    cri_a = np.round(cri_a)\n",
    "    return IrcTuple(int(cri_a[2]), int(cri_a[1]), int(cri_a[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **# 3** `from util.logconf import logging`\n",
    "\n",
    "据 Copilot 的说法, `logging` 是被直接导入的，而不是 `logconf` 模块的一部分。所以，如果你想在其他地方使用 `logging`，你应该直接导入它，就像这样：`import logging`。\n",
    "\n",
    "---\n",
    "\n",
    "#### 更新\n",
    "\n",
    "对话如下:\n",
    "\n",
    "**You:** 有没有可能是这段代码修改了部分关于logging的相关参数?\n",
    "\n",
    "**Copilot:** \n",
    "\n",
    "> 是的，你完全正确。这段代码实际上是在配置Python的`logging`模块。\n",
    "> \n",
    "> 首先，它获取了根日志记录器（`root_logger`），并设置了其日志级别为`INFO`。这意味着只有级别为`INFO`或更高级别的日志消息才会被处理。\n",
    "> \n",
    "> 然后，它移除了所有已经添加到根日志记录器的处理器。这是因为有些库可能会尝试添加自己的处理器，这可能会干扰我们的日志记录。\n",
    "> \n",
    "> 接下来，它定义了一个日志格式字符串（`logfmt_str`），并创建了一个格式器（`formatter`）。这个格式器会按照这个格式字符串来格式化日志消息。\n",
    "> \n",
    "> 然后，它创建了一个流处理器（`streamHandler`），并设置了其格式器和日志级别。这个流处理器会将日志消息输出到标准输出流（通常是控制台）。\n",
    "> \n",
    "> 最后，它将这个流处理器添加到根日志记录器。这样，当我们使用`logging`模块来记录日志时，日志消息就会被这个流处理器处理，并按照我们设置的格式输出到标准输出流。\n",
    "> \n",
    "> 所以，当你在其他地方使用`from util.logconf import logging`时，你实际上是在导入这个已经被配置过的`logging`模块。\n",
    "\n",
    "由此, 以下代码是必要的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import logging.handlers\n",
    "\n",
    "root_logger = logging.getLogger()\n",
    "root_logger.setLevel(logging.INFO)\n",
    "\n",
    "# Some libraries attempt to add their own root logger handlers. This is\n",
    "# annoying and so we get rid of them.\n",
    "for handler in list(root_logger.handlers):\n",
    "    root_logger.removeHandler(handler)\n",
    "\n",
    "logfmt_str = \"%(asctime)s %(levelname)-8s pid:%(process)d %(name)s:%(lineno)03d:%(funcName)s %(message)s\"\n",
    "formatter = logging.Formatter(logfmt_str)\n",
    "\n",
    "streamHandler = logging.StreamHandler()\n",
    "streamHandler.setFormatter(formatter)\n",
    "streamHandler.setLevel(logging.DEBUG)\n",
    "\n",
    "root_logger.addHandler(streamHandler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = ('/home/ben/Datasets/luna16/')\n",
    "sublist = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CandidateInfoTuple = namedtuple(\n",
    "    'CandidateInfoTuple',\n",
    "    'isNodule_bool, diameter_mm, series_uid, center_xyz',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCandidateInfoList(requireOnDisk_bool=True):\n",
    "    mhd_list = list(chain.from_iterable([glob.glob(path + f'subset{i}/*.mhd') for i in range(sublist)]))\n",
    "    presentOnDisk_set = {os.path.split(p)[-1][:-4] for p in mhd_list}\n",
    "\n",
    "    diameter_dict = {}\n",
    "    with open(path + 'annotations.csv', \"r\") as f:\n",
    "        for row in list(csv.reader(f))[1:]:\n",
    "            series_uid = row[0]\n",
    "            annotationCenter_xyz = tuple([float(x) for x in row[1:4]])\n",
    "            annotationDiameter_mm = float(row[4])\n",
    "\n",
    "            diameter_dict.setdefault(series_uid, []).append(\n",
    "                (annotationCenter_xyz, annotationDiameter_mm)\n",
    "            )\n",
    "\n",
    "    candidateInfo_list = []\n",
    "    with open(path + 'candidates.csv', \"r\") as f:\n",
    "        for row in list(csv.reader(f))[1:]:\n",
    "            series_uid = row[0]\n",
    "\n",
    "            if series_uid not in presentOnDisk_set and requireOnDisk_bool:\n",
    "                continue\n",
    "\n",
    "            isNodule_bool = bool(int(row[4]))\n",
    "            candidateCenter_xyz = tuple([float(x) for x in row[1:4]])\n",
    "\n",
    "            candidateDiameter_mm = 0.0\n",
    "            for annotation_tup in diameter_dict.get(series_uid, []):\n",
    "                annotationCenter_xyz, annotationDiameter_mm = annotation_tup\n",
    "                for i in range(3):\n",
    "                    delta_mm = abs(candidateCenter_xyz[i] - annotationCenter_xyz[i])\n",
    "                    if delta_mm > annotationDiameter_mm / 4:\n",
    "                        break\n",
    "                else:\n",
    "                    candidateDiameter_mm = annotationDiameter_mm\n",
    "                    break\n",
    "\n",
    "            candidateInfo_list.append(CandidateInfoTuple(\n",
    "                isNodule_bool,\n",
    "                candidateDiameter_mm,\n",
    "                series_uid,\n",
    "                candidateCenter_xyz,\n",
    "            ))\n",
    "\n",
    "    candidateInfo_list.sort(reverse=True)\n",
    "    return candidateInfo_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ct:\n",
    "    def __init__(self, series_uid):\n",
    "        # mhd_path = glob.glob(\n",
    "        #     'data-unversioned/part2/luna/subset*/{}.mhd'.format(series_uid)\n",
    "        # )[0]\n",
    "        mhd_path = list(\n",
    "            chain.from_iterable(\n",
    "                glob.glob(\n",
    "                    path + f'subset{i}/{series_uid}.mhd'\n",
    "                ) for i in range(sublist)\n",
    "            )\n",
    "        )[0]\n",
    "        ct_mhd = sitk.ReadImage(mhd_path)\n",
    "        ct_a = np.array(sitk.GetArrayFromImage(ct_mhd), dtype=np.float32)\n",
    "\n",
    "        ct_a.clip(-1000, 1000, ct_a)\n",
    "\n",
    "        self.series_uid = series_uid\n",
    "        self.hu_a = ct_a\n",
    "\n",
    "        self.origin_xyz = XyzTuple(*ct_mhd.GetOrigin())\n",
    "        self.vxSize_xyz = XyzTuple(*ct_mhd.GetSpacing())\n",
    "        self.direction_a = np.array(ct_mhd.GetDirection()).reshape(3, 3)\n",
    "\n",
    "    def getRawCandidate(self, center_xyz, width_irc):\n",
    "        center_irc = xyz2irc(\n",
    "            center_xyz,\n",
    "            self.origin_xyz,\n",
    "            self.vxSize_xyz,\n",
    "            self.direction_a,\n",
    "        )\n",
    "\n",
    "        slice_list = []\n",
    "        for axis, center_val in enumerate(center_irc):\n",
    "            start_ndx = int(round(center_val - width_irc[axis]/2))\n",
    "            end_ndx = int(start_ndx + width_irc[axis])\n",
    "\n",
    "            if start_ndx < 0:\n",
    "                start_ndx = 0\n",
    "                end_ndx = int(width_irc[axis])\n",
    "\n",
    "            if end_ndx > self.hu_a.shape[axis]:\n",
    "                end_ndx = self.hu_a.shape[axis]\n",
    "                start_ndx = int(self.hu_a.shape[axis] - width_irc[axis])\n",
    "\n",
    "            slice_list.append(slice(start_ndx, end_ndx))\n",
    "\n",
    "        ct_chunk = self.hu_a[tuple(slice_list)]\n",
    "\n",
    "        return ct_chunk, center_irc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCt(series_uid):\n",
    "    return Ct(series_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCtRawCandidate(series_uid, center_xyz, width_irc):\n",
    "    ct = getCt(series_uid)\n",
    "    ct_chunk, center_irc = ct.getRawCandidate(center_xyz, width_irc)\n",
    "    return ct_chunk, center_irc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LunaDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 val_stride=0,\n",
    "                 isValSet_bool=None,\n",
    "                 series_uid=None,\n",
    "            ):\n",
    "        self.candidateInfo_list = copy.copy(getCandidateInfoList())\n",
    "\n",
    "        if series_uid:\n",
    "            self.candidateInfo_list = [\n",
    "                x for x in self.candidateInfo_list if x.series_uid == series_uid\n",
    "            ]\n",
    "\n",
    "        if isValSet_bool:\n",
    "            assert val_stride > 0, val_stride\n",
    "            self.candidateInfo_list = self.candidateInfo_list[::val_stride]\n",
    "            assert self.candidateInfo_list\n",
    "        elif val_stride > 0:\n",
    "            del self.candidateInfo_list[::val_stride]\n",
    "            assert self.candidateInfo_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.candidateInfo_list)\n",
    "\n",
    "    def __getitem__(self, ndx):\n",
    "        candidateInfo_tup = self.candidateInfo_list[ndx]\n",
    "        width_irc = (32, 48, 48)\n",
    "\n",
    "        candidate_a, center_irc = getCtRawCandidate(\n",
    "            candidateInfo_tup.series_uid,\n",
    "            candidateInfo_tup.center_xyz,\n",
    "            width_irc,\n",
    "        )\n",
    "\n",
    "        candidate_t = torch.from_numpy(candidate_a)\n",
    "        candidate_t = candidate_t.to(torch.float32)\n",
    "        candidate_t = candidate_t.unsqueeze(0)\n",
    "\n",
    "        pos_t = torch.tensor([\n",
    "                not candidateInfo_tup.isNodule_bool,\n",
    "                candidateInfo_tup.isNodule_bool\n",
    "            ],\n",
    "            dtype=torch.long,\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            candidate_t,\n",
    "            pos_t,\n",
    "            candidateInfo_tup.series_uid,\n",
    "            torch.tensor(center_irc),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `RUN dataset()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = LunaDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 48, 48])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0][0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
