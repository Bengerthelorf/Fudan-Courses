{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '2_CT.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import collections\n",
    "import copy\n",
    "import csv\n",
    "import functools\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from collections import namedtuple\n",
    "from itertools import chain\n",
    "import SimpleITK as sitk\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import logging.handlers\n",
    "\n",
    "root_logger = logging.getLogger()\n",
    "root_logger.setLevel(logging.INFO)\n",
    "\n",
    "# Some libraries attempt to add their own root logger handlers. This is\n",
    "# annoying and so we get rid of them.\n",
    "for handler in list(root_logger.handlers):\n",
    "    root_logger.removeHandler(handler)\n",
    "\n",
    "logfmt_str = \"%(asctime)s %(levelname)-8s pid:%(process)d %(name)s:%(lineno)03d:%(funcName)s %(message)s\"\n",
    "formatter = logging.Formatter(logfmt_str)\n",
    "\n",
    "streamHandler = logging.StreamHandler()\n",
    "streamHandler.setFormatter(formatter)\n",
    "streamHandler.setLevel(logging.DEBUG)\n",
    "\n",
    "root_logger.addHandler(streamHandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def irc2xyz(coord_irc, origin_xyz, vxSize_xyz, direction_a):\n",
    "    cri_a = np.array(coord_irc)[::-1]\n",
    "    origin_a = np.array(origin_xyz)\n",
    "    vxSize_a = np.array(vxSize_xyz)\n",
    "    coords_xyz = (direction_a @ (cri_a * vxSize_a)) + origin_a\n",
    "    # coords_xyz = (direction_a @ (idx * vxSize_a)) + origin_a\n",
    "    return XyzTuple(*coords_xyz)\n",
    "\n",
    "def xyz2irc(coord_xyz, origin_xyz, vxSize_xyz, direction_a):\n",
    "    origin_a = np.array(origin_xyz)\n",
    "    vxSize_a = np.array(vxSize_xyz)\n",
    "    coord_a = np.array(coord_xyz)\n",
    "    cri_a = ((coord_a - origin_a) @ np.linalg.inv(direction_a)) / vxSize_a\n",
    "    cri_a = np.round(cri_a)\n",
    "    return IrcTuple(int(cri_a[2]), int(cri_a[1]), int(cri_a[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCandidateInfoList(requireOnDisk_bool=True):\n",
    "    # We construct a set with all series_uids that are present on disk.\n",
    "    # This will let us use the data, even if we haven't downloaded all of\n",
    "    # the subsets yet.\n",
    "    mhd_list = list(chain.from_iterable([glob.glob(path + f'subset{i}/*.mhd') for i in range(sublist)]))\n",
    "    presentOnDisk_set = {os.path.split(p)[-1][:-4] for p in mhd_list}\n",
    "\n",
    "    diameter_dict = {}\n",
    "    with open(path + 'annotations.csv', \"r\") as f:\n",
    "        for row in list(csv.reader(f))[1:]:\n",
    "            series_uid = row[0]\n",
    "            annotationCenter_xyz = tuple([float(x) for x in row[1:4]])\n",
    "            annotationDiameter_mm = float(row[4])\n",
    "\n",
    "            diameter_dict.setdefault(series_uid, []).append(\n",
    "                (annotationCenter_xyz, annotationDiameter_mm)\n",
    "            )\n",
    "\n",
    "    candidateInfo_list = []\n",
    "    with open(path + 'candidates.csv', \"r\") as f:\n",
    "        for row in list(csv.reader(f))[1:]:\n",
    "            series_uid = row[0]\n",
    "\n",
    "            if series_uid not in presentOnDisk_set and requireOnDisk_bool:\n",
    "                continue\n",
    "\n",
    "            isNodule_bool = bool(int(row[4]))\n",
    "            candidateCenter_xyz = tuple([float(x) for x in row[1:4]])\n",
    "\n",
    "            candidateDiameter_mm = 0.0\n",
    "            for annotation_tup in diameter_dict.get(series_uid, []):\n",
    "                annotationCenter_xyz, annotationDiameter_mm = annotation_tup\n",
    "                for i in range(3):\n",
    "                    delta_mm = abs(candidateCenter_xyz[i] - annotationCenter_xyz[i])\n",
    "                    if delta_mm > annotationDiameter_mm / 4:\n",
    "                        break\n",
    "                else:\n",
    "                    candidateDiameter_mm = annotationDiameter_mm\n",
    "                    break\n",
    "\n",
    "            candidateInfo_list.append(CandidateInfoTuple(\n",
    "                isNodule_bool,\n",
    "                candidateDiameter_mm,\n",
    "                series_uid,\n",
    "                candidateCenter_xyz,\n",
    "            ))\n",
    "\n",
    "    candidateInfo_list.sort(reverse=True)\n",
    "    return candidateInfo_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ct:\n",
    "    def __init__(self, series_uid):\n",
    "        mhd_path = glob.glob(\n",
    "            list(chain.from_iterable(glob.glob(path + f'subset{i}/{series_uid}.mhd') for i in range(2)))[0]\n",
    "        )[0]\n",
    "\n",
    "        ct_mhd = sitk.ReadImage(mhd_path)\n",
    "        ct_a = np.array(sitk.GetArrayFromImage(ct_mhd), dtype=np.float32)\n",
    "\n",
    "        # CTs are natively expressed in https://en.wikipedia.org/wiki/Hounsfield_scale\n",
    "        # HU are scaled oddly, with 0 g/cc (air, approximately) being -1000 and 1 g/cc (water) being 0.\n",
    "        # The lower bound gets rid of negative density stuff used to indicate out-of-FOV\n",
    "        # The upper bound nukes any weird hotspots and clamps bone down\n",
    "        ct_a.clip(-1000, 1000, ct_a)\n",
    "\n",
    "        self.series_uid = series_uid\n",
    "        self.hu_a = ct_a\n",
    "\n",
    "        self.origin_xyz = XyzTuple(*ct_mhd.GetOrigin())\n",
    "        self.vxSize_xyz = XyzTuple(*ct_mhd.GetSpacing())\n",
    "        self.direction_a = np.array(ct_mhd.GetDirection()).reshape(3, 3)\n",
    "\n",
    "    def getRawCandidate(self, center_xyz, width_irc):\n",
    "        center_irc = xyz2irc(\n",
    "            center_xyz,\n",
    "            self.origin_xyz,\n",
    "            self.vxSize_xyz,\n",
    "            self.direction_a,\n",
    "        )\n",
    "\n",
    "        slice_list = []\n",
    "        for axis, center_val in enumerate(center_irc):\n",
    "            start_ndx = int(round(center_val - width_irc[axis]/2))\n",
    "            end_ndx = int(start_ndx + width_irc[axis])\n",
    "\n",
    "            assert center_val >= 0 and center_val < self.hu_a.shape[axis], repr([self.series_uid, center_xyz, self.origin_xyz, self.vxSize_xyz, center_irc, axis])\n",
    "\n",
    "            if start_ndx < 0:\n",
    "                # log.warning(\"Crop outside of CT array: {} {}, center:{} shape:{} width:{}\".format(\n",
    "                #     self.series_uid, center_xyz, center_irc, self.hu_a.shape, width_irc))\n",
    "                start_ndx = 0\n",
    "                end_ndx = int(width_irc[axis])\n",
    "\n",
    "            if end_ndx > self.hu_a.shape[axis]:\n",
    "                # log.warning(\"Crop outside of CT array: {} {}, center:{} shape:{} width:{}\".format(\n",
    "                #     self.series_uid, center_xyz, center_irc, self.hu_a.shape, width_irc))\n",
    "                end_ndx = self.hu_a.shape[axis]\n",
    "                start_ndx = int(self.hu_a.shape[axis] - width_irc[axis])\n",
    "\n",
    "            slice_list.append(slice(start_ndx, end_ndx))\n",
    "\n",
    "        ct_chunk = self.hu_a[tuple(slice_list)]\n",
    "\n",
    "        return ct_chunk, center_irc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LunaDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 val_stride,\n",
    "                 isValSet_bool = None,\n",
    "                 series_uid=None\n",
    "            ):\n",
    "        self.candidateInfo_list = copy.copy(getCandidateInfoList())\n",
    "\n",
    "        if series_uid:\n",
    "            self.candidateInfo_list = [\n",
    "                x for x in self.candidateInfo_list if x.series_uid == series_uid\n",
    "            ]\n",
    "        \n",
    "        if isValSet_bool:\n",
    "            assert val_stride > 0, val_stride\n",
    "            self.candidateInfo_list = self.candidateInfo_list[::val_stride]\n",
    "            assert self.candidateInfo_list\n",
    "        \n",
    "        elif val_stride > 0:\n",
    "            del self.candidateInfo_list[::val_stride]\n",
    "            assert self.candidateInfo_list\n",
    "        \n",
    "        log.info(\"{!r}: {} {} samples\".format(\n",
    "            self,\n",
    "            len(self.candidateInfo_list),\n",
    "            \"validation\" if isValSet_bool else \"training\"\n",
    "        ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.candidateInfo_list)\n",
    "    \n",
    "    def __getitem__(self, ndx):\n",
    "        candidateInfo_tup = self.candidateInfo_list[ndx]\n",
    "        width_irc = (32, 48, 48)\n",
    "\n",
    "        candidate_a, center_irc = getCtRawCandidate(\n",
    "            candidateInfo_tup.series_uid,\n",
    "            candidateInfo_tup.center_xyz,\n",
    "            width_irc\n",
    "        )\n",
    "\n",
    "        # candidate_t = candidate_a.astype(np.float32)\n",
    "        candidate_t = torch.from_numpy(candidate_t)\n",
    "        candidate_t = candidate_t.to(torch.float32)\n",
    "        candidate_t = candidate_t.unsqueeze(0)\n",
    "\n",
    "        pos_t = torch.tensor([\n",
    "            not candidateInfo_tup.isNodule_bool,\n",
    "            candidateInfo_tup.isNodule_bool\n",
    "        ], dtype=torch.long)\n",
    "        \n",
    "        return (\n",
    "            candidate_t,\n",
    "            pos_t,\n",
    "            candidateInfo_tup.series_uid,\n",
    "            torch.tensor(center_irc)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = ('/home/ben/Datasets/luna16/')\n",
    "sublist = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LunaDataset.__init__() missing 1 required positional argument: 'val_stride'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mLunaDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m ds[\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: LunaDataset.__init__() missing 1 required positional argument: 'val_stride'"
     ]
    }
   ],
   "source": [
    "ds = LunaDataset()\n",
    "ds[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
