{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1eeeafc550>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.set_printoptions(edgeitems=2)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "data_path = '../../data-unversioned/p1ch7/'\n",
    "cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar2 = [(img, label_map[label])\n",
    "          for img, label in cifar10 \n",
    "          if label in [0, 2]]\n",
    "\n",
    "cifar2_val = [(img, label_map[label])\n",
    "              for img, label in cifar10_val\n",
    "              if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 10000\n",
      "10000 2000\n"
     ]
    }
   ],
   "source": [
    "print(len(cifar10), len(cifar10_val))\n",
    "print(len(cifar2), len(cifar2_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "            else torch.device('cpu'))\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar2 = [(img.to(device),\n",
    "           torch.tensor(label).to(device)) for img, label in cifar2]\n",
    "cifar2_val = [(img.to(device), \n",
    "               torch.tensor(label).to(device)) for img, label in cifar2_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 原\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.710599\n",
      "Epoch: 1, Loss: 0.398297\n",
      "Epoch: 2, Loss: 0.265748\n",
      "Epoch: 3, Loss: 0.477997\n",
      "Epoch: 4, Loss: 0.347228\n",
      "Epoch: 5, Loss: 0.298988\n",
      "Epoch: 6, Loss: 0.326244\n",
      "Epoch: 7, Loss: 0.267870\n",
      "Epoch: 8, Loss: 0.296569\n",
      "Epoch: 9, Loss: 0.216771\n",
      "Epoch: 10, Loss: 0.340905\n",
      "Epoch: 11, Loss: 0.412462\n",
      "Epoch: 12, Loss: 0.094640\n",
      "Epoch: 13, Loss: 0.106730\n",
      "Epoch: 14, Loss: 0.106087\n",
      "Epoch: 15, Loss: 0.107549\n",
      "Epoch: 16, Loss: 0.259040\n",
      "Epoch: 17, Loss: 0.128283\n",
      "Epoch: 18, Loss: 0.120966\n",
      "Epoch: 19, Loss: 0.312628\n",
      "Epoch: 20, Loss: 0.254893\n",
      "Epoch: 21, Loss: 0.214502\n",
      "Epoch: 22, Loss: 0.015418\n",
      "Epoch: 23, Loss: 0.130515\n",
      "Epoch: 24, Loss: 0.221259\n",
      "Epoch: 25, Loss: 0.003098\n",
      "Epoch: 26, Loss: 0.035281\n",
      "Epoch: 27, Loss: 0.054112\n",
      "Epoch: 28, Loss: 0.001782\n",
      "Epoch: 29, Loss: 0.002782\n",
      "Epoch: 30, Loss: 0.008732\n",
      "Epoch: 31, Loss: 0.201580\n",
      "Epoch: 32, Loss: 0.025544\n",
      "Epoch: 33, Loss: 0.048837\n",
      "Epoch: 34, Loss: 0.041420\n",
      "Epoch: 35, Loss: 0.002884\n",
      "Epoch: 36, Loss: 0.011901\n",
      "Epoch: 37, Loss: 0.028004\n",
      "Epoch: 38, Loss: 0.001116\n",
      "Epoch: 39, Loss: 0.009907\n",
      "Epoch: 40, Loss: 0.018576\n",
      "Epoch: 41, Loss: 0.017040\n",
      "Epoch: 42, Loss: 0.083856\n",
      "Epoch: 43, Loss: 0.000247\n",
      "Epoch: 44, Loss: 0.027188\n",
      "Epoch: 45, Loss: 0.001505\n",
      "Epoch: 46, Loss: 0.051550\n",
      "Epoch: 47, Loss: 0.007205\n",
      "Epoch: 48, Loss: 0.002592\n",
      "Epoch: 49, Loss: 0.008370\n",
      "Epoch: 50, Loss: 0.002031\n",
      "Epoch: 51, Loss: 0.000743\n",
      "Epoch: 52, Loss: 0.032123\n",
      "Epoch: 53, Loss: 0.034032\n",
      "Epoch: 54, Loss: 0.000186\n",
      "Epoch: 55, Loss: 0.023768\n",
      "Epoch: 56, Loss: 0.000445\n",
      "Epoch: 57, Loss: 0.010161\n",
      "Epoch: 58, Loss: 0.005295\n",
      "Epoch: 59, Loss: 0.003449\n",
      "Epoch: 60, Loss: 0.000114\n",
      "Epoch: 61, Loss: 0.000212\n",
      "Epoch: 62, Loss: 0.057316\n",
      "Epoch: 63, Loss: 0.005935\n",
      "Epoch: 64, Loss: 0.171774\n",
      "Epoch: 65, Loss: 0.003415\n",
      "Epoch: 66, Loss: 0.000017\n",
      "Epoch: 67, Loss: 0.000798\n",
      "Epoch: 68, Loss: 0.000186\n",
      "Epoch: 69, Loss: 0.000230\n",
      "Epoch: 70, Loss: 0.000387\n",
      "Epoch: 71, Loss: 0.001110\n",
      "Epoch: 72, Loss: 0.000110\n",
      "Epoch: 73, Loss: 0.000146\n",
      "Epoch: 74, Loss: 0.000436\n",
      "Epoch: 75, Loss: 0.000531\n",
      "Epoch: 76, Loss: 0.000119\n",
      "Epoch: 77, Loss: 0.000241\n",
      "Epoch: 78, Loss: 0.000124\n",
      "Epoch: 79, Loss: 0.000020\n",
      "Epoch: 80, Loss: 0.000056\n",
      "Epoch: 81, Loss: 0.000050\n",
      "Epoch: 82, Loss: 0.000042\n",
      "Epoch: 83, Loss: 0.000108\n",
      "Epoch: 84, Loss: 0.000166\n",
      "Epoch: 85, Loss: 0.000324\n",
      "Epoch: 86, Loss: 0.000041\n",
      "Epoch: 87, Loss: 0.000034\n",
      "Epoch: 88, Loss: 0.000075\n",
      "Epoch: 89, Loss: 0.000076\n",
      "Epoch: 90, Loss: 0.000041\n",
      "Epoch: 91, Loss: 0.000146\n",
      "Epoch: 92, Loss: 0.000072\n",
      "Epoch: 93, Loss: 0.000016\n",
      "Epoch: 94, Loss: 0.000008\n",
      "Epoch: 95, Loss: 0.000228\n",
      "Epoch: 96, Loss: 0.000011\n",
      "Epoch: 97, Loss: 0.000023\n",
      "Epoch: 98, Loss: 0.000042\n",
      "Epoch: 99, Loss: 0.000016\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Linear(3072, 1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 2)).to(device)\n",
    "\n",
    "learning_rate = 1e-1\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in train_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        \n",
    "print(\"Accuracy: %f\" % (correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
    "                                         shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        \n",
    "print(\"Accuracy: %f\" % (correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSEloss\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(imgs.view(imgs.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 2]), torch.Size([64]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5359, 0.4641],\n",
       "        [0.4836, 0.5164],\n",
       "        [0.5043, 0.4957],\n",
       "        [0.4864, 0.5136],\n",
       "        [0.4710, 0.5290],\n",
       "        [0.4459, 0.5541],\n",
       "        [0.5290, 0.4710],\n",
       "        [0.4828, 0.5172],\n",
       "        [0.4198, 0.5802],\n",
       "        [0.4818, 0.5182],\n",
       "        [0.4852, 0.5148],\n",
       "        [0.5307, 0.4693],\n",
       "        [0.4654, 0.5346],\n",
       "        [0.4806, 0.5194],\n",
       "        [0.4740, 0.5260],\n",
       "        [0.5418, 0.4582],\n",
       "        [0.4456, 0.5544],\n",
       "        [0.4469, 0.5531],\n",
       "        [0.4351, 0.5649],\n",
       "        [0.5383, 0.4617],\n",
       "        [0.5484, 0.4516],\n",
       "        [0.4696, 0.5304],\n",
       "        [0.5315, 0.4685],\n",
       "        [0.4968, 0.5032],\n",
       "        [0.4634, 0.5366],\n",
       "        [0.4909, 0.5091],\n",
       "        [0.4730, 0.5270],\n",
       "        [0.4619, 0.5381],\n",
       "        [0.4765, 0.5235],\n",
       "        [0.5477, 0.4523],\n",
       "        [0.4784, 0.5216],\n",
       "        [0.5023, 0.4977],\n",
       "        [0.4612, 0.5388],\n",
       "        [0.5264, 0.4736],\n",
       "        [0.5429, 0.4571],\n",
       "        [0.4825, 0.5175],\n",
       "        [0.4882, 0.5118],\n",
       "        [0.5456, 0.4544],\n",
       "        [0.4866, 0.5134],\n",
       "        [0.4521, 0.5479],\n",
       "        [0.5300, 0.4700],\n",
       "        [0.4569, 0.5431],\n",
       "        [0.4677, 0.5323],\n",
       "        [0.5460, 0.4540],\n",
       "        [0.4988, 0.5012],\n",
       "        [0.4563, 0.5437],\n",
       "        [0.4601, 0.5399],\n",
       "        [0.4951, 0.5049],\n",
       "        [0.4865, 0.5135],\n",
       "        [0.5323, 0.4677],\n",
       "        [0.5079, 0.4921],\n",
       "        [0.5183, 0.4817],\n",
       "        [0.5094, 0.4906],\n",
       "        [0.5096, 0.4904],\n",
       "        [0.4437, 0.5563],\n",
       "        [0.4893, 0.5107],\n",
       "        [0.5432, 0.4568],\n",
       "        [0.4713, 0.5287],\n",
       "        [0.5380, 0.4620],\n",
       "        [0.5374, 0.4626],\n",
       "        [0.4950, 0.5050],\n",
       "        [0.5150, 0.4850],\n",
       "        [0.4422, 0.5578],\n",
       "        [0.5319, 0.4681]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要先换label成outputs对应的1,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (64) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m imgs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     24\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(imgs\u001b[38;5;241m.\u001b[39mview(imgs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 25\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     28\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/loss.py:536\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/functional.py:3294\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3292\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3294\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/functional.py:74\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (64) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Linear(3072, 1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 2),\n",
    "            nn.Softmax(dim=1)).to(device)\n",
    "\n",
    "learning_rate = 1e-1\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
